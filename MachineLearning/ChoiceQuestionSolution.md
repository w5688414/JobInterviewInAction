## p1
```
假设我们有一个如下图所示的隐藏层。隐藏层在这个网络中起到了一定的降维作用。假如现在我们用另一种维度下降的方法，比如说主成分分析法(PCA)来替代这个隐藏层。那么，这两者的输出效果是一样的吗？
```
```
不同，因为PCA用于相关特征而隐层用于有预测能力的特征。
```

## p2
```
神经网络能组成函数(y=1x)吗？
```
```
可以，因为激活函数可以是互反函数。
```

## p3
```
假设在训练中我们突然遇到了一个问题，在几次循环之后，误差瞬间降低
你认为数据有问题，于是你画出了数据并且发现也许是数据的偏度过大造成了这个问题。 
你打算怎么做来处理这个问题？
```
```
首先您将相关的数据去掉，然后将其置零。具体来说，误差瞬间降低, 一般原因是多个数据样本有强相关性且突然被拟合命中, 或者含有较大方差数据样本突然被拟合命中. 所以对数据作主成分分析(PCA)和归一化能够改善这个问题。
```
- reference
BAT机器学习面试1000题系列（第150~279题）:https://cloud.tencent.com/developer/article/1057059

## p4
```
在SPSS的基础分析模块中，作用是“以行列表的形式揭示数据之间的关系”的是
```
```
交叉表 
```
## p5
```
SPSS中，数据整理的功能主要集中在（数据 和 转换 ）等菜单中。
```

## p6
```
特征比数据量还大时，选择什么样的分类器？
```
```
线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。
```
## p7
```
关于线性回归的描述,以下正确的有: （ACEF）
A. 基本假设包括随机干扰项是均值为0,方差为1的标准正态分布 
B. 基本假设包括随机干扰下是均值为0的同方差正态分布 
C. 在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量 
D. 在违背基本假设时,模型不再可以估计 
E. 可以用DW检验残差是否存在序列相关性 
F. 多重共线性会使得参数估计值方差减小

正确答案：（ACEF）

```

```
1、AB一元线性回归的基本假设有： 
　　（1）随机误差项是一个期望值或平均值为0的随机变量； 
　　（2）对于解释变量的所有观测值，随机误差项有相同的方差； 
　　（3）随机误差项彼此不相关； 
　　（4）解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立； 
　　（5）解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵； 
　　（6）随机误差项服从正态分布 
2、CD 违背基本假设的计量经济学模型还是可以估计的，只是不能使用普通最小二乘法进行估计。 
当存在异方差时，普通最小二乘法估计存在以下问题： 参数估计值虽然是无偏的，但不是最小方差线性无偏估计。 
3、E杜宾-瓦特森（DW）检验，计量经济，统计分析中常用的一种检验序列一阶自相关最常用的方法。
4、F所谓多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。影响 
　　（1）完全共线性下参数估计量不存在 
　　（2）近似共线性下OLS估计量非有效 
多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀因子(Variance Inflation Factor, VIF) 
　　（3）参数估计量经济含义不合理 
　　（4）变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外 
　　（5）模型的预测功能失效。变大的方差容易使区间预测的“区间”变大，使预测失去意义。 
　　对于线性回归模型,当响应变量服从正态分布,误差项满足高斯–马尔科夫条件（零均值、等方差、不相关）时,回归参数的最小二乘估计是一致最小方差无偏估计。 
　　当然，该条件只是理想化的假定，为的是数学上有相应的较为成熟的结论。其实大多数实际问题都不完全满足这些理想化的假定。 
　　线性回归模型理论的发展正是在不断克服理想化条件不被满足时得到许多新方法。如加权LSE、岭估计、压缩估计、BOX_COX变换等一系列段。做实际工作时一定是要超越书本上的理想化条件的。
```

## p8
```
已知一组数据的协方差矩阵P,下面关于主分量说法错误的是( C )
A. 主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小 
B. 在经主分量分解后,协方差矩阵成为对角矩阵 
C. 主分量分析就是K-L变换 
D. 主分量是通过求协方差矩阵的特征值得到
```
```
K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。
```
## p9
```
在统计模式识分类问题中，当先验概率未知时，可以使用( BC )?
A. 最小损失准则 
B. N-P判决 
C. 最小最大损失准则 
D. 最小误判概率准则
```
```
选项 A ,最小损失准则中需要用到先验概率

选项B ,在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。 
　　1. p(y)已知，直接使用贝叶斯公式求后验概率即可； 
　　2. p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。 
　　聂曼-皮尔逊决策（N-P判决）可以归结为找阈值a，即： 
　　如果p（x|w1）/p（x|w2）>a，则 x属于w1； 
　　如果p（x|w1）/p（x|w2）<a，则 x属于w2；

选项C ,最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的问题的。

```
## p10
```
下面哪些是基于核的机器学习算法?( BCD )
A. Expectation Maximization（EM）（最大期望算法） 
B. Radial Basis Function（RBF）（径向基核函数） 
C. Linear Discrimimate Analysis（LDA）（主成分分析法） 
D. Support Vector Machine（SVM）（支持向量机）
```
```
径向基核函数是非常常用的核函数，而主成分分析法的常规方法是线性的，但是当遇到非线性的时候，同样可以使用核方法使得非线性问题转化为线性问题。支持向量机处理非线性的问题的时候，核函数也是非常重要的。
```

## p11
```
对数几率回归（logistics regression）和一般回归分析有什么区别？ D
A. 对数几率回归是设计用来预测事件可能性的 
B. 对数几率回归可以用来度量模型拟合程度 
C. 对数几率回归可以用来估计回归系数 
D. 以上所有
```
```
A: 对数几率回归其实是设计用来解决分类问题的 
B: 对数几率回归可以用来检验模型对数据的拟合度 
C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。
```
## p12
```
在一个n维的空间中， 最好的检测outlier(离群点)的方法是：（C）
A. 作正态分布概率图 
B. 作盒形图 
C. 马氏距离 
D. 作散点图
```
```
马氏距离是基于卡方分布的，度量多元outlier离群点的统计方法。
```

## p13
```
“过拟合”只在监督学习中出现，在非监督学习中，没有”过拟合”，这是：（B）
A. 对的 
B. 错的 

```
```
答案：（B）
我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score）。
```

## p14
```
对于k折交叉验证, 以下对k的说法正确的是 :（D）

A. k越大, 不一定越好, 选择大的k会加大评估时间 
B. 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集) 
C. 在选择k时, 要最小化数据集之间的方差 
D. 以上所有
```
```
k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差
```
## p15
```
回归模型中存在多重共线性, 你如何解决这个问题？
1.去除这两个共线性变量 
2.我们可以先去除一个共线性变量 
3.计算VIF(方差膨胀因子), 采取相应措施 
4.为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归. 
以下哪些是对的：（D）

A. 1 
B. 2 
C. 2和3 
D. 2, 3和4
```
```
解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值<=4说明相关性不是很高, VIF值>=10说明相关性较高. 
我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。
```

## p16
```
假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值, 那么现在关于模型说法, 正确的是 : （C）

1.模型分类的召回率会降低或不变 
2.模型分类的召回率会升高 
3.模型分类准确率会升高或不变 
4.模型分类准确率会降低

A. 1 
B. 2 
C.1和3 
D. 2和4 
E. 以上都不是
```
- reference
Confidence Splitting Criterions Can Improve Precision And Recall in Random Forest Classifiers. https://medium.com/airbnb-engineering/confidence-splitting-criterions-can-improve-precision-and-recall-in-random-forest-classifiers-ad2d4ba696a4

## p17
```
对于PCA(主成分分析)转化过的特征 , 朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 :（B）

A. 正确的 
B. 错误的
```
```
这个说法是错误的。首先，“不依赖”和“不相关”是两回事；其次, 转化过的特征, 也可能是相关的。
```

## p18
```
对应GradientBoosting tree算法， 以下说法正确的是 :（C） 
1.当增加最小样本分裂个数，我们可以抵制过拟合 
2.当增加最小样本分裂个数，会导致过拟合 
3.当我们减少训练单个学习器的样本个数，我们可以降低variance 
4.当我们减少训练单个学习器的样本个数，我们可以降低bias
A. 2 和 4 
B. 2 和 3 
C. 1 和 3 
D. 1 和 4

```
```
最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。第二点是考bias和variance概念的。
```
## p19
```
对于线性回归模型，包括附加变量在内，以下的可能正确的是 :（D） 
1.R-Squared 和 Adjusted R-squared都是递增的 
2.R-Squared 是常量的，Adjusted R-squared是递增的 
3.R-Squared 是递减的， Adjusted R-squared 也是递减的 
4.R-Squared 是递减的， Adjusted R-squared是递增的

A. 1 和 2 
B. 1 和 3 
C. 2 和 4 
D. 以上都不是
```
```
R-Squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-Squared有R-Squared和predicted R-Squared所没有的问题。每次为模型加入预测器，R-Squared递增或者不变。
```

## p20
```
一 般的，如果相关系数大于0.7或者小于-0.7，是高相关的。
相关系数的范围应该是[-1,1]。
```
## p21
```
下面有关序列模式挖掘算法的描述，错误的是？（C）

A. AprioriAll算法和GSP算法都属于Apriori类算法，都要产生大量的候选序列 
B. FreeSpan算法和PrefixSpan算法不生成大量的候选序列以及不需要反复扫描原数据库 
C. 在时空的执行效率上，FreeSpan比PrefixSpan更优 
D. 和AprioriAll相比，GSP的执行效率比较高
```
```
2. GPS算法：类Apriori算法。用于从候选项集中发现具有时序先后性的频繁项集。两个步骤：进行自连接、进行剪枝。缺点：每次计算支持度，都需要扫描全部数据集；对序列模式很长的情况，由于其对应的短的序列模式规模太大，算法很难处理。

3. SPADE算法：改进的GPS算法，规避多次对数据集D进行全表扫描的问题。与GSP算法大体相同，多了一个ID_LIST记录，使得每一次的ID_LIST根据上一次的ID_LIST得到（从而得到支持度）。而ID_LIST的规模是随着剪枝的不断进行而缩小的。所以也就解决了GSP算法多次扫描数据集D问题。

4.  FreeSpan算法：即频繁模式投影的序列模式挖掘。核心思想是分治算法。基本思想为：利用频繁项递归地将序列数据库投影到更小的投影数据库集中，在每个投影数据库中生成子序列片断。这一过程对数据和待检验的频繁模式集进行了分割，并且将每一次检验限制在与其相符合的更小的投影数据库中。

优点：减少产生候选序列所需的开销。缺点：可能会产生许多投影数据库，开销很大，会产生很多的

5. PrefixSpan 算法：从FreeSpan中推导演化而来的。收缩速度比FreeSpan还要更快些。
```
- reference
机器学习：序列模式挖掘算法. https://blog.csdn.net/ztf312/article/details/50889238

## p22
```
类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是？(D)

A. 伪逆法-径向基（RBF）神经网络的训练算法，就是解决线性不可分的情况 
B. 基于二次准则的H-K算法：最小均方差准则下求得权矢量，二次准则解决非线性问题 
C. 势函数法－非线性 
D. 感知器算法－线性分类算法
```

## p23
```
下列哪些方法可以用来对高维数据进行降维（A B C D E F）

A. LASSO 
B. 主成分分析法 
C. 聚类分析 
D. 小波分析法 
E. 线性判别法 
F. 拉普拉斯特征映射

```
```
LASSO通过参数缩减达到降维的目的； 
PCA就不用说了； 
线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维； 
小波分析有一些变换的操作降低其他干扰可以看做是降维；
```
- reference
机器学习降维算法四：Laplacian Eigenmaps 拉普拉斯特征映射. https://blog.csdn.net/xbinworld/article/details/8855796

## p24
```
1、SVM对噪声（如来自其他分布的噪声样本）鲁棒       错
SVM（支持向量机）本身对噪声具有一定的鲁棒性，但实验证明，是当噪声率低于一定水平（如40%）时噪声对SVM没有太大影响，算法仍有效，但随着噪声率的不断增加，分类器的识别率会降低。http://www.docin.com/p-749158537.html

拓展：SVM在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。

在机器学习中，支持向量机（Support Vector Machine，还支持矢量网络）是与相关的学习算法有关的监督学习模型，可以分析数据，识别模式，用于分类和回归分析。

2、在AdaBoost算法中，所有被分错的样本的权重更新比例相同  对
AdaBoost算法中不同的训练集是通过调整每个样本对应的权重来实现的。开始时，每个样本对应的权重是相同的，即其中 n 为样本个数，在此样本分布下训练出一弱分类器。对于分类错误的样本，加大其对应的权重；而对于分类正确的样本，降低其权重，这样分错的样本就被突显出来，从而得到一个新的样本分布。在新的样本分布下，再次对样本进行训练，得到弱分类器。依次类推，经过 T 次循环，得到 T 个弱分类器，把这 T 个弱分类器按一定的权重叠加（boost）起来，得到最终想要的强分类器。
AdaBoost算法的具体步骤如下：

1. 给定训练样本集S，其中X和Y分别对应于正例样本和负例样本； T为训练的最大循环次数；

2. 初始化样本权重为1/n ，即为训练样本的初始概率分布；

3. 第一次迭代：

(1) 训练样本的概率分布相当下，训练弱分类器；

(2) 计算弱分类器的错误率；

(3) 选取合适阈值，使得误差最小；

(4) 更新样本权重；

经T次循环后，得到T个弱分类器，按更新的权重叠加，最终得到的强分类器。

Adaboost算法是经过调整的Boosting算法，其能够对弱学习得到的弱分类器的错误进行适应性调整。每一次迭代，都要对权重进行更新。更新的规则是：减小弱分类器分类效果较好的数据的概率，增大弱分类器分类效果较差的数据的概率。最终的分类器是个弱分类器的加权平均。

3、Boost和Bagging都是组合多个分类器投票的方法，二者均是根据单个分类器的正确率决定其权重          错 
 Boost和Bagging都是组合多个弱分类器投票的方法，但二者均是根据单个分类器的正确率决定其权重的说法错误

Bagging与Boosting的区别：二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的各轮训练集的选择与前面各轮的学习结果有关

投票方法（Voting，也叫组合分类器，使用在文本分类中）就是一种典型的集成机器学习方法。它通过组合多个弱分类器来得到一个强分类器，包括Bagging和Boosting两种方式，二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。投票分类方法虽然分类精度较高，但训练时间较长。

Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成。

拓展：bagging和boosting都可以有效地提高分类的准确性。在大多数数据集中，boosting的准确性比bagging高。在有些数据集中，boosting会引起退化--- Overfitting(过拟合)。

Boosting思想的一种改进型AdaBoost方法在邮件过滤、文本分类方面都有很好的性能。

4、给定n 个数据点，如果其中一半用于训练，一半用于测试，则训练误差与测试误差之间的差别会随着n 的增加而减少         对
```
- reference
搜狗2016研发工程师笔试题中有关于机器学习的几个判断题及解析. http://www.mamicode.com/info-detail-1208462.html

## p25
```
以下关于PMF(概率质量函数),PDF(概率密度函数),CDF(累积分布函数)描述错误的是？（A）

A. PDF描述的是连续型随机变量在特定取值区间的概率 
B. CDF是PDF在特定区间上的积分 
C. PMF描述的是离散型随机变量在特定取值点的概率 
D. 有一个分布的CDF函数H(x),则H(a)等于P(X<=a)
```
```
概率质量函数 (probability mass function，PMF)是离散随机变量在各特定取值上的概率。 
概率密度函数（probability density function，PDF ）是对连续随机变量定义的，本身不是概率，只有对连续随机变量的取值进行积分后才是概率。 
累积分布函数（cumulative distribution function，CDF） 能完整描述一个实数随机变量X的概率分布，是概率密度函数的积分。对于所有实数x 与pdf相对。
```
## p26
```
线性回归的基本假设有哪些？(ABDE)

A. 随机误差项是一个期望值为0的随机变量； 
B. 对于解释变量的所有观测值，随机误差项有相同的方差； 
C. 随机误差项彼此相关； 
D. 解释变量是确定性变量不是随机变量，与随机误差项之间相互独立； 
E. 随机误差项服从正态分布
```
## p27
```
下面哪些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是正确的？(A、C）

A. 类型 1 通常称之为假正类，类型 2 通常称之为假负类。 
B. 类型 2 通常称之为假正类，类型 1 通常称之为假负类。 
C. 类型 1 错误通常在其是正确的情况下拒绝假设而出现。
 
在统计学假设测试中，I 类错误即错误地拒绝了正确的假设即假正类错误，II 类错误通常指错误地接受了错误的假设即假负类错误。
```
## p28
```
模式为对数据集的全局性总结，它对整个测量空间的每一点做出描述；模型则对变量变化空间的一个有限区域做出描述。（错）
寻找模式和规则主要是对数据进行干扰，使其符合某种规则以及模式。（错）
离群点可以是合法的数据对象或者值。　　　　（对）
特征提取技术并不依赖于特定的领域。　　　　　　（错）
可视化技术对于分析的数据类型通常不是专用性的。　　　　（错）
DSS主要是基于数据仓库.联机数据分析和数据挖掘技术的应用。（对）
OLAP技术侧重于把数据库中的数据进行分析、转换成辅助决策信息，是继数据库技术发展之后迅猛发展起来的一种新技术。 （对）
数据仓库中间层OLAP服务器只能采用关系型OLAP （错）
数据仓库系统的组成部分包括数据仓库，仓库管理，数据抽取，分析工具等四个部分. (错)
Web数据挖掘是通过数据库中的一些属性来预测另一个属性,它在验证用户提出的假设过程中提取信息. （错）
关联规则挖掘过程是发现满足最小支持度的所有项集代表的规则。（错）
利用先验原理可以帮助减少频繁项集产生时需要探查的候选项个数（对）。
具有较高的支持度的项集具有较高的置信度。（错）
聚类（clustering）是这样的过程：它找出描述并区分数据类或概念的模型(或函数)，以便能够使用模型预测类标记未知的对象类。 （错）
分类(classification )是这样的过程：它找出描述并区分数据类或概念的模型(或函数)，以便能够使用模型预测类标记未知的对象类。
基于邻近度的离群点检测方法不能处理具有不同密度区域的数据集。（对）
如果一个对象不强属于任何簇，那么该对象是基于聚类的离群点。（对）
从点作为个体簇开始，每一步合并两个最接近的簇，这是一种分裂的层次聚类方法。（错）
DBSCAN是相对抗噪声的，并且能够处理任意形状和大小的簇。（对）
```

## p29
```
使用交互式的和可视化的技术，对数据进行探索属于数据挖掘的哪一类任务？
探索性数据分析

为数据的总体分布建模；把多维空间划分成组等问题属于数据挖掘的哪一类任务？
建模描述

建立一个模型，通过这个模型根据已知的变量值来预测其他某个变量值属于数据挖掘的哪一类任务？
预测建模

用户有一种感兴趣的模式并且希望在数据集中找到相似的模式，属于数据挖掘哪一类任务？
根据内容检索

假设12个销售价格记录已经排序，如下所示： 5,10,11,13,15,35,50,55,72,92,204,215 使用如下各方法将它们划分成三个箱。 (a)等频（等深）划分。 (b)等宽划分。 (c)聚类。

等深的箱: 箱1： 5, 10, 11, 13 箱2： 15, 35, 50, 55 箱3： 72, 92, 204, 215 
等宽划分的箱： (215-5 / 3) = 70
箱1： 5, 10, 11, 13, 15, 35, 50, 55, 72 箱2： 92 箱3： 204, 215 聚类的箱： 箱1： 5, 10, 11, 13, 15 箱2： 35, 50, 55, 72, 92 箱3： 204, 215

```
```
 只有非零值才重要的二元属性被称作：( C )

A 计数属性

B 离散属性

C非对称的二元属性

D 对称属性
```
```
下面不属于创建新属性的相关方法的是： (B)

A特征提取

B特征修改

C映射数据到新的空间

D特征构造

考虑值集{1、2、3、4、5、90}，其截断均值（p=20%）是 (3.5) ？

6* （20/2）%=0.6 约等于1
去掉1和90，剩下的算均值为（2+3+4+5）/4=3.5

截断均值 在统计学里面一般是去除最高端的5%和最低端的5%。当然为了满足不同的需求，不一定是5%，但是一般都是高端和低端同时去除同样比例的数据
目的主要是为了避免部分极高值和极低值对于数据整体均值的影响，从而使平均值对整体更加有代表性
```

## p30
```
假定用于分析的数据包含属性age。数据元组中age的值如下（按递增序）：13，15，16，16，19，20，20，21，22，22，25，25，25，30，33，33，35，35，36，40，45，46，52，70, 问题：使用按箱平均值平滑方法对上述数据进行平滑，箱的深度为3。第二个箱子

值为：(A)

A 18.3

B 22.6

C 26.8

D 27.9


```
- reference
数据挖掘笔记（三）—数据预处理. https://blog.csdn.net/lizhengnanhua/article/details/8982968

## p31
```
考虑值集{12， 24， 33， 24， 55， 68， 26}，其四分位数极差是 31
```
```
四分位数(Quartile)，即统计学中，把所有数值由小到大排列并分成四等份，处于三个分割点位置的数值就是四分位数。

Q1的位置= (n+1) × 0.25

Q2的位置= (n+1) × 0.5

Q3的位置= (n+1) × 0.75

n表示项数

第1个和第3个四分位数之间的距离是散布的一种简单度量，它给出被数据的中间一半所覆盖的范围。该距离称为四分位数极差(IQR)，定义为
IQR = Q3 - Q1

排序后：12 ，24, 24, 26，33 ,55,68

Q1位置：（7+1）*0.25 = 2 对应数24

Q3位置：（7+1)*0.75 = 6 对应数55

四分位数极差55-24 = 31

```

## p32
```
在抽样方法中，当合适的样本容量很难确定时，可以使用的抽样方法是： (D)

A 有放回的简单随机抽样

B 无放回的简单随机抽样

C 分层抽样

D 渐进抽样

数据仓库是随着时间变化的,下面的描述不正确的是 (C)

A. 数据仓库随时间的变化不断增加新的数据内容;

B. 捕捉到的新数据会覆盖原来的快照;

C. 数据仓库随事件变化不断删去旧的数据内容;

D. 数据仓库中包含大量的综合数据,这些综合数据会随着时间的变化不断地进行重新综合.


```

## p33
```
OLAP技术的核心是:多维分析.
关于OLAP的特性,下面正确的是:(1)快速性 (2)可分析性 (3)多维性 (4)信息性 (5)共享性
基本元数据包括关于装载和更新处理,分析处理以及管理方面的信息.
数据仓库开发要从数据出发;
数据仓库使用的需求在开发出去就要明确;
数据仓库的开发是一个不断循环的过程,是启发式的开发;
OLAM服务器通过用户图形借口接收用户的分析指令,在元数据的知道下,对超级立方体作一定的操作.
```
```
关于OLAP和OLTP的区别描述,不正确的是: (C)

A. OLAP主要是关于如何理解聚集的大量不同的数据.它与OTAP应用程序不同.

B. 与OLAP应用程序不同,OLTP应用程序包含大量相对简单的事务.

C. OLAP的特点在于事务量大,但事务内容比较简单且重复率高.

D. OLAP是以数据仓库为基础的,但其最终数据来源与OLTP一样均来自底层的数据库系统,两者面对的用户是相同的.

概念分层图是__(B)__图。

A、无向无环

B、有向无环

C、有向有环

D、无向有环

在图集合中发现一组公共子结构，这样的任务称为 ( B )

A、频繁子集挖掘

B、频繁子图挖掘

C、频繁数据项挖掘

D、频繁模式挖掘

下列度量不具有反演性的是 (D)

A、系数

B、几率

C、Cohen度量

D、兴趣因子

以下哪项关于决策树的说法是错误的 (C)

A. 冗余属性不会对决策树的准确率造成不利的影响

B. 子树可能在决策树中重复多次

C. 决策树算法对于噪声的干扰非常敏感

D. 寻找最佳决策树是NP完全问题

```
## p34
```
如果对属性值的任一组合，R中都存在一条规则加以覆盖，则称规则集R中的规则为(B)

A, 无序规则

B，穷举规则

C， 互斥规则

D，有序规则

如果允许一条记录触发多条分类规则，把每条被触发规则的后件看作是对相应类的一次投票，然后计票确定测试记录的类标号，称为（A）

A, 无序规则

B，穷举规则

C， 互斥规则

D，有序规则

```
```
凝聚层次聚类：

   所谓凝聚的，指的是该算法初始时，将每个点作为一个簇，每一步合并两个最接近的簇。另外即使到最后，对于噪音点或是离群点也往往还是各占一簇的，除非过度合并。对于这里的“最接近”，有下面三种定义。我在实现是使用了MIN，该方法在合并时，只要依次取当前最近的点对，如果这个点对当前不在一个簇中，将所在的两个簇合并就行：

    (1)单链(MIN):定义簇的邻近度为不同两个簇的两个最近的点之间的距离。

    (2)全链(MAX):定义簇的邻近度为不同两个簇的两个最远的点之间的距离。

    (3)组平均：定义簇的邻近度为取自两个不同簇的所有点对邻近度的平均值。

根据该算法，实现如下代码。开始时计算每个点对的距离，并按距离降序依次合并。另外为了防止过度合并，定义的退出条件是90%的簇被合并，即当前簇数是初始簇数的10%：
```
- reference
聚类算法:凝聚层次聚类. https://blog.csdn.net/yt_sports/article/details/50456859

## p35
```
说出物体检测、人脸识别、物体分割等某一领域的常见算法，并用一两句话简述其中一种算法的原理
```
```
物体检测：YOLO、R-CNN、Fast R-CNN、Faster R-CNN、Mask R-CNN、SSD
YOLO 算法直接利用一个卷积神经网络就输出物体所在的位置以及所属的类别，是一个端到端的系统，因此检测速度特别快，可以达到实时性的要求。
```
## p36
```
正则化方法
数据增强(Data Augmentation)、L1 正则化、L2 正则化、Dropout、Drop Connect 和早停(Early stop)
```
## p37
```
Dropout率和正规化有什么关系？
提示：我们定义Dropout率为保留一个神经元为激活状态的概率.

高dropout率意味着更多神经元是激活的，所以这亦为之正规化更少。
```
## p38
```
普通反向传播算法和随时间的反向传播算法（BPTT）有什么技术上的不同？
与普通反向传播不同的是，BPTT会在每个时间步长内叠加所有对应权重的梯度。
```
## p39
```
有许多种梯度下降算法，其中两种最出名的方法是l-BFGS和SGD。l-BFGS根据二阶梯度下降而SGD是根据一阶梯度下降的。
在下述哪些场景中，会更加偏向于使用l-BFGS而不是SGD？
场景1： 数据很稀疏
场景2： 神经网络的参数数量较少

在这两种情况下，l-BFGS的效果都是最好的
```
## p40
```
对于非连续目标在深度神经网络的优化过程中，下面哪种梯度下降方法是最好的？
拉格朗日松弛Subgradient method
优化算法无法作用于非连续目标。
```
## p41
```
在构建一个神经网络时，batch size通常会选择2的次方，比如256和512。这是为什么呢？

当内存使用最优时这可以方便神经网络并行化
```
## p42
```
Xavier初始化是最为常用的神经网络权重初始化方法，下图是初始化的公式。
Xavier初始化是用来帮助信号能够在神经网络中传递得更深，下面哪些叙述是对的？
1 如果权重一开始很小，信号到达最后也会很小
2 如果权重一开始很大，信号到达最后也会很大
3 Xavier初始化是由高斯发布引出的
4 Xavier初始化可以帮助减少梯度弥散问题
Var(W)=2/(n_in+n_out)
```
## p43
```
Dropout对于循环层效果并不理想，你可能需要稍微修改一下dropout技术来得到良好的结果。
```
## p44
```
当在内存网络中获得某个内存空间时，通常选择读取矢量形式数据而不是标量，这里需要的哪种类型的寻址来完成？
基于内容的寻址
```
## p45
```
一般我们建议将卷积生成对抗网络（convolutional generative adversarial nets）中生成部分的池化层替换成什么？
局部跨距卷积层(Fractional strided convolutional layer)

```
[How to Train a GAN? Tips and tricks to make GANs work][2]

[2]: https://github.com/soumith/ganhacks

## p46
```
在CNN中使用1×1卷积时，下列哪一项是正确的？
它可以帮助降低维数
可以用于特征池
由于小的内核大小，它会减少过拟合

1×1卷积在CNN中被称为bottleneck structure。

即使所有的偏差都为零，神经网络也有可能学习。
另一方面，如果所有的权重都是零; 神经网络可能永远不会学习执行任务。

多义现象可以被定义为在文本对象中一个单词或短语的多种含义共存。
下列哪一种方法可能是解决此问题的最好选择？
CNN 是文本分类问题中比较受欢迎的选择，因为它们把上下文的文本当作特征来考虑，这样可以解决多义问题。
```
## p47
```
Word2vec，为一群用来产生词向量的相关模型。这些模型为浅而双层的神经网络，用来训练以重新建构语言学之词文本。网络以词表现，并且需猜测相邻位置的输入词，在word2vec中词袋模型假设下，词的顺序是不重要的。

训练完成之后，word2vec模型可用来映射每个词到一个向量，可用来表示词对词之间的关系。该向量为神经网络之隐藏。
Word2vec依赖skip-grams或连续词袋（CBOW）来建立神经词嵌入。
统计学中的泛化误差是指对模型对未知数据的预测能力。
```
## p48
```
假定你使用了一个很大γ值的RBF核，这意味着：

SVM调参中的γ衡量距离超平面远近的点的影响。

对于较小的γ，模型受到严格约束，会考虑训练集中的所有点，而没有真正获取到数据的模式、对于较大的γ，模型能很好地学习到模型。

代价参数决定着SVM能够在多大程度上适配训练数据。
如果你想要一个平稳的决策平面，代价会比较低；如果你要将更多的数据正确分类，代价会比较高。可以简单的理解为误分类的代价。

假定你使用SVM学习数据X，数据X里面有些点存在错误。现在如果你使用一个二次核函数，多项式阶数为2，使用松弛变量C作为超参之一。
当你使用较大的C（C趋于无穷），则：

采用更大的C，误分类点的惩罚就更大，因此决策边界将尽可能完美地分类数据。

假定你使用SVM学习数据X，数据X里面有些点存在错误。现在如果你使用一个二次核函数，多项式阶数为2，使用松弛变量C作为超参之一。
如果使用较小的C（C趋于0），则：
分类器会最大化大多数点之间的间隔，少数点会误分类，因为惩罚太小了。

SVM广泛应用于实际问题中，包括回归，聚类，手写数字识别等。

假设你训练SVM后，得到一个线性决策边界，你认为该模型欠拟合。假如你想修改SVM的参数，同样达到模型不会欠拟合的效果，应该怎么做？
增大参数C会得到正则化模型
```
## p49
```
判断：没有必要有一个用于应用维数降低算法的目标变量。
LDA是有监督降维算法的一个例子。

以下哪种技术对于减少数据集的维度会更好？
如果列的缺失值太多（例如99％），那么可以删除这些列。

最常用的降维算法是PCA，以下哪项是关于PCA的？
1.PCA是一种无监督的方法
2.它搜索数据具有最大差异的方向
3.主成分的最大数量<=特征能数量
4.所有主成分彼此正交

9、假设使用维数降低作为预处理技术，使用PCA将数据减少到k维度。然后使用这些PCA预测作为特征，以下哪个声明是正确的？
较高的k导致较少的平滑，因此能够保留更多的数据特征，从而减少正则化。 

在相同的机器上运行并设置最小的计算能力，以下哪种情况下t-SNE比PCA降维效果更好？
t-SNE具有二次时空复杂度。
具有10,000项8个特征的数据集
```
## p50
```
SNE代价函数是不对称的，这使得使用梯度下降难以收敛。对称是SNE和t-SNE代价函数之间的主要区别之一。 
t-SNE代表t分布随机相邻嵌入，它考虑最近的邻居来减少数据。
t-SNE学习非参数映射，这意味着它不会学习将数据从输入空间映射到地图的显式函数。

在t-SNE算法中，可以调整以下哪些超参数？
维度数量,平稳测量有效数量的邻居,最大迭代次数

Xi和Xj是较高维度表示中的两个不同点，其中Yi和Yj是较低维度中的Xi和Xj的表示。
1.数据点Xi与数据点Xj的相似度是条件概率p（j | i）。
2.数据点Yi与数据点Yj的相似度是条件概率q（j | i）。
对于在较低维度空间中的Xi和Xj的完美表示，以下哪一项必须是正确的？

两点的相似性的条件概率必须相等，因为点之间的相似性必须在高维和低维中保持不变，以使它们成为完美的表示。
```
```
对于投影数据为(( √2)，(0)，(√2))。现在如果在二维空间中重建，并将它们视为原始数据点的重建，那么重建误差是多少？
重建误差为0，因为所有三个点完全位于第一个主要分量的方向上或者计算重建;
```
```
1、以下哪种情况LDA会失败？
如果有辨识性的信息是数据的均值和方差

6、以下哪一项给出了逻辑回归与LDA之间的差异？
1. 如果类别分离好，逻辑回归的参数估计可能不稳定。
2. 如果样本量小，并且每个类的特征分布是正常的。在这种情况下，线性判别分析比逻辑回归更稳定。

假设正在处理10类分类问题，并且想知道LDA最多可以产生几个判别向量。以下哪个是正确答案？ 9

LDA最多产生c-1个判别向量。
```
## p51
```
Logistic Regression是一个分类算法，所以它的输出不能是实值，所以均方误差不能用于评估它。
如果学习速率低下，代价函数将缓慢下降，学习速度过高，则其代价函数会迅速下降。

AIC信息准则即Akaike information criterion，是衡量统计模型拟合优良性的一种标准，由于它为日本统计学家赤池弘次创立和发展的，因此又称赤池信息量准则。

考虑到AIC=2k-2In(L) ，所以一般而言，当模型复杂度提高（k增大）时，似然函数L也会增大，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度（极大似然），而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。

综上，我们一般选择逻辑回归中最少的AIC作为最佳模型。

逻辑回归不需要标准化。功能标准化的主要目标是帮助优化技术组合。

```
```
4、选择Logistic回归中的One-Vs-All方法中的哪个选项是真实的。
A
我们需要在n类分类问题中适合n个模型
B
我们需要适合n-1个模型来分类为n个类
C
我们需要只适合1个模型来分类为n个类
D
这些都没有

如果存在n个类，那么n个单独的逻辑回归必须与之相适应，其中每个类的概率由剩余类的概率之和确定。这题答案选A，但是我还没搞懂为啥，以后再看看。
```
## p52
```
抛掷一枚正常硬币，正面朝上的几率（odds）为多少？
几率（odds）是事件发生不发生概率的比率，正面朝上概率为1/2和反面朝上的概率都为1/2，所以几率为1。

Logit函数（给定为l（x））是几率函数的对数。域x = [0,1]中logit函数的范围是多少？
为了与目标相适应，几率函数具有将值从0到1的概率函数变换成值在0和∞之间的等效函数的优点。当我们采用几率函数的自然对数时，我们便能范围是-∞到∞的值。

这道题我不是很懂

```
## p53
```
3、假设对给定数据应用了Logistic回归模型，并获得了训练精度X和测试精度Y。现在要在同一数据中添加一些新特征，以下哪些是错误的选项。
注：假设剩余参数相同。
A
训练精度提高
B
训练准确度提高或保持不变
C
测试精度提高或保持不变

将更多的特征添加到模型中会增加训练精度，因为模型必须考虑更多的数据来适应逻辑回归。但是，如果发现特征显着，则测试精度将会增加。
```
```
6、假定特征 F1 可以取特定值：A、B、C、D、E 和 F，其代表着学生在大学所获得的评分。
在下面说法中哪一项是正确的？

特征 F1 是有序变量（ordinal variable）的一个实例。

有序变量是一种在类别上有某些顺序的变量。例如，等级 A 就要比等级 B 所代表的成绩好一些。
```
## p54
```
在统计学假设测试中，I 类错误即错误地拒绝了正确的假设（即假正类错误），II 类错误通常指错误地接受了错误的假设（即假负类错误）。
t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。
当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。
```
```
8、假设存在一个黑箱算法，其输入为有多个观察（t1, t2, t3,…….. tn）的训练数据和一个新的观察（q1）。该黑箱算法输出 q1 的最近邻 ti 及其对应的类别标签 ci。你可以将这个黑箱算法看作是一个 1-NN（1-最近邻）
能够仅基于该黑箱算法而构建一个 k-NN 分类算法？注：相对于 k 而言，n（训练观察的数量）非常大。

在第一步，你在这个黑箱算法中传递一个观察样本 q1，使该算法返回一个最近邻的观察样本及其类别，在第二步，你在训练数据中找出最近观察样本，然后再一次输入这个观察样本（q1）。该黑箱算法将再一次返回一个最近邻的观察样本及其类别。你需要将这个流程重复 k 次。
```
## p55
```
回归的残差之和一定为零，故而平均值也为零
在误差项中，非恒定方差的存在导致了异方差性。一般来说，非恒定方差的出现时因为异常值或极端杠杆值的存在。
变量间的相关系数为0说明了变量间的较强关系；另一方面，p-value和t统计量仅仅衡量了非零联系的证据有多强。在数据足够多的情况下，哪怕弱影响都可能是显著的。
```
```
在导出线性回归的参数时，我们做出下列哪种假定？
1.因变量y和自变量x的真实关系是线性的
2.模型误差是统计独立的
3.误差通常服从一个平均值为零，标准差恒定的分布
4.自变量x是非随机的，无错的

答案是以上都对
```
## p56
```
假设你在训练一个线性回归模型，以下哪项是正确的？
1.数据越少越易过拟合
2.假设区间小则易过拟合

1.小训练数据集更容易找到过拟合训练数据的假设
2.从偏差和方差的权衡中可以看出，假设区间小，偏差更大，方差更小。所以在小假设区间的情况下，不太可能找到欠拟合数据的假设
```
## p57
```
假设我们用Lasso回归拟合一个有100个特征值(X1,X2…X100)的数据集，现在，我们重新调节其中一个值，将它乘10（将它视作X1），并再次拟合同一规则化参数。下列哪一项正确？ B
A X1很可能被模型排除
B X1很可能被包含在模型内
C 很难说
D 都不对

大特征值=⇒小相关系数=⇒更少lasso penalty =⇒更可能被保留
```
```
每次加入特征值，R方总是上升或维持不变。但调整R方并非如此，当它上升时，特征值是显著的。
```
```
7、下列哪项可以评价回归模型？
1.R方
2.调整R方
3.F统计量
4.RMSE/MSE/MAE
以上这些都是评价回归模型的指标
```
## p58
```
9、Y的预期值是关于变量X(X1,X2….Xn)的线性函数，回归线定义为Y = β0 + β1 X1 + β2 X2……+ βn Xn，下列陈述哪项正确？
1.如果Xi的变化量为 ∆Xi，其它为常量，则Y的变化量为βi ∆Xi，常量βi可以为正数或负数
2. βi 的值都是一样的，除非是其它X的βi
3.X对Y预期值的总影响为每个分影响之和
提示：特征值间相互独立，互不干扰 D

A 1和2
B 1和3
C 2和3
D 1,2和3

Y的预期值是关于X的线性函数，这表示：
如果Xi的变化量为 ∆Xi，其它变量不变，Y的预期值随β i ∆X i而变化，β i可以为正数或负数
βi 的值都是一样的，除非是其它X的βi
X对Y预期值的总影响为每个分影响之和
Y的未知变化独立于随机变量（特别之处，当随机变量为时间序列时，Y与随机变量不是自动关联的）
它们的方差一致（同方差性）
它们一般是分散的
```
```
1.相关是衡量两个变量线性联系的统计度量，对待x、y是对称的
2.回归是用于根据x预测y，其关系不对称
斜率不是直接和平均值中位数相关的
```
## p59
```
关于Ridge回归，下列哪项正确？ A
1.lambda为0时，模型作用类似于线性回归模型
2.lambda为0时，模型作用与线性回归模型不相像
3.当lambda趋向无穷，会得到非常小，趋近0的相关系数
4.当lambda趋向无穷，会得到非常大，趋近无穷的相关系数


A 1和3
B 1和4
C 2和3
D 2和4
当lambda为0时我们得到了最小的最小二乘解；当lambda趋近无穷时，会得到非常小、趋近0的相关系数。
```
```
10、考虑线性回归和逻辑回归中的重量/相关系数，关于cost函数的偏导，下列哪一项是正确的？ B
A 都不一样
B 都一样
C 无法确定
D 以上都不对
```
## p60
```
一般来说，电影推荐系统会基于用户过往的活动和资料，将用户聚集在有限数量的相似组中。然后，从根本上来说，对同一集群的用户进行相似的推荐。

在某些情况下，电影推荐系统也可以归为分类问题，将最适当的某类电影分配给特定用户组的用户。与此同时，电影推荐系统也可以视为增强学习问题，即通过先前的推荐来改进以后的电影推荐。

马氏距离是是一种有效的计算两个未知样本集的相似度的多元计量方法，以卡方分布为基础，表示数据的协方差距离。与欧氏距离不同的是它考虑到各种特性之间的联系(例如：一条关于身高的信息会带来一条关于体重的信息，因为两者是关联的)。因此马氏距离常用于多元异常值检测。 

为了检查多重共线性，我们可以创建相关系数矩阵来辨别和移除相关系数大于75%的变量(阈值根据情况设定),除此之外，我们可以使用VIF方法来检查当前存在的共线变量。VIF<=4表明没有多种共线，VIF>=10表明有着严重的多重共线性。当然，我们也可以使用公差(tolerance)作为评估指标。
但是,移除相关变量可能导致信息的丢失，为了保留这些变量，我们可以使用带惩罚的回归方法。我们也可以在相关变量之间随机加入噪音，使得变量之间存在差异。但增加噪音可能影响准确度，因此这种方法应该小心使用。
```
## p61
```
4、一个SVM存在欠拟合问题，下面怎么做能提高模型的性能: A
A 增大惩罚参数C
B 减小惩罚参数C
C 减小核函数系数(gamma值)

C >0称为惩罚参数，是调和二者的系数，C值大时对误差分类的惩罚增大，当C越大，趋近无穷的时候，表示不允许分类误差的存在，margin越小，容易过拟合。

C值小时对误差分类的惩罚减小,当C趋于0时，表示我们不再关注分类是否正确，只要求margin越大，容易欠拟合。
```
```
假设你正在做一个项目，它是一个二元分类问题。你在数据集上训练一个模型，并在验证数据集上得到混淆矩阵。基于上述混淆矩阵，下面哪个选项会给你正确的预测。
1 精确度是~0.91
2 错误分类率是~0.91
3 假正率（False correct classification）是~0.95
4 真正率（True positive rate）是~0.95
```
```
11、以下属于欧式距离特性的有（） A
A
旋转不变性
B
尺度缩放不变性
C
不受量纲影响的特性
```
```
17、对数几率回归（logistics regression）和一般回归分析有什么区别？D
A 对数几率回归是设计用来预测事件可能性的
B 对数几率回归可以用来度量模型拟合程度
C 对数几率回归可以用来估计回归系数
D 以上所有

A: 对数几率回归其实是设计用来解决分类问题的
B: 对数几率回归可以用来检验模型对数据的拟合度
C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。
```
```
假设在训练中我们突然遇到了一个问题，在几次循环之后，误差瞬间降低
你认为数据有问题，于是你画出了数据并且发现也许是数据的偏度过大造成了这个问题。

首先您将相关的数据去掉，然后将其置零。具体来说，误差瞬间降低, 一般原因是多个数据样本有强相关性且突然被拟合命中, 或者含有较大方差数据样本突然被拟合命中. 所以对数据作主成分分析(PCA)和归一化能够改善这个问题。
```
```
22、混沌度(Perplexity)是一种常见的应用在使用深度学习处理NLP问题过程中的评估技术，关于混沌度，哪种说法是正确的？ B
A 混沌度没什么影响
B 混沌度越低越好
C 混沌度越高越好
D 混沌度对于结果的影响不一定
```
```
16、假设你正在做一个项目，它是一个二元分类问题。你在数据集上训练一个模型，并在验证数据集上得到混淆矩阵。基于上述混淆矩阵，下面哪个选项会给你正确的预测。
1 精确度是~0.91
2 错误分类率是~0.91
3 假正率（False correct classification）是~0.95
4 真正率（True positive rate）是~0.95
A 1 和 3
B 2 和 4
C 1 和 4
D 2 和 3

精确度（正确分类）是 (50+100)/165，约等于 0.91。真正率是你正确预测正分类的次数，因此真正率将是 100/105 = 0.95，也被称作敏感度或召回。
```
```
对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：
TP——将正类预测为正类数
FN——将正类预测为负类数
FP——将负类预测为正类数
TN——将负类预测为负类数
由此：
精准率定义为：P = TP / (TP + FP)
召回率定义为：R = TP / (TP + FN)
F1值定义为： F1 = 2 P R / (P + R)
精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。
```
```
Lasso不允许闭式解，L1-penalty使解为非线性的，所以需要近似解。
```