## p1
```
假设我们有一个如下图所示的隐藏层。隐藏层在这个网络中起到了一定的降维作用。假如现在我们用另一种维度下降的方法，比如说主成分分析法(PCA)来替代这个隐藏层。那么，这两者的输出效果是一样的吗？
```
```
不同，因为PCA用于相关特征而隐层用于有预测能力的特征。
```

## p2
```
神经网络能组成函数(y=1x)吗？
```
```
可以，因为激活函数可以是互反函数。
```

## p3
```
假设在训练中我们突然遇到了一个问题，在几次循环之后，误差瞬间降低
你认为数据有问题，于是你画出了数据并且发现也许是数据的偏度过大造成了这个问题。 
你打算怎么做来处理这个问题？
```
```
首先您将相关的数据去掉，然后将其置零。具体来说，误差瞬间降低, 一般原因是多个数据样本有强相关性且突然被拟合命中, 或者含有较大方差数据样本突然被拟合命中. 所以对数据作主成分分析(PCA)和归一化能够改善这个问题。
```
- reference
BAT机器学习面试1000题系列（第150~279题）:https://cloud.tencent.com/developer/article/1057059

## p4
```
在SPSS的基础分析模块中，作用是“以行列表的形式揭示数据之间的关系”的是
```
```
交叉表 
```
## p5
```
SPSS中，数据整理的功能主要集中在（数据 和 转换 ）等菜单中。
```

## p6
```
特征比数据量还大时，选择什么样的分类器？
```
```
线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。
```
## p7
```
关于线性回归的描述,以下正确的有: （ACEF）
A. 基本假设包括随机干扰项是均值为0,方差为1的标准正态分布 
B. 基本假设包括随机干扰下是均值为0的同方差正态分布 
C. 在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量 
D. 在违背基本假设时,模型不再可以估计 
E. 可以用DW检验残差是否存在序列相关性 
F. 多重共线性会使得参数估计值方差减小

正确答案：（ACEF）

```

```
1、AB一元线性回归的基本假设有： 
　　（1）随机误差项是一个期望值或平均值为0的随机变量； 
　　（2）对于解释变量的所有观测值，随机误差项有相同的方差； 
　　（3）随机误差项彼此不相关； 
　　（4）解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立； 
　　（5）解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵； 
　　（6）随机误差项服从正态分布 
2、CD 违背基本假设的计量经济学模型还是可以估计的，只是不能使用普通最小二乘法进行估计。 
当存在异方差时，普通最小二乘法估计存在以下问题： 参数估计值虽然是无偏的，但不是最小方差线性无偏估计。 
3、E杜宾-瓦特森（DW）检验，计量经济，统计分析中常用的一种检验序列一阶自相关最常用的方法。
4、F所谓多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。影响 
　　（1）完全共线性下参数估计量不存在 
　　（2）近似共线性下OLS估计量非有效 
多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀因子(Variance Inflation Factor, VIF) 
　　（3）参数估计量经济含义不合理 
　　（4）变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外 
　　（5）模型的预测功能失效。变大的方差容易使区间预测的“区间”变大，使预测失去意义。 
　　对于线性回归模型,当响应变量服从正态分布,误差项满足高斯–马尔科夫条件（零均值、等方差、不相关）时,回归参数的最小二乘估计是一致最小方差无偏估计。 
　　当然，该条件只是理想化的假定，为的是数学上有相应的较为成熟的结论。其实大多数实际问题都不完全满足这些理想化的假定。 
　　线性回归模型理论的发展正是在不断克服理想化条件不被满足时得到许多新方法。如加权LSE、岭估计、压缩估计、BOX_COX变换等一系列段。做实际工作时一定是要超越书本上的理想化条件的。
```

## p8
```
已知一组数据的协方差矩阵P,下面关于主分量说法错误的是( C )
A. 主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小 
B. 在经主分量分解后,协方差矩阵成为对角矩阵 
C. 主分量分析就是K-L变换 
D. 主分量是通过求协方差矩阵的特征值得到
```
```
K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。
```
## p9
```
在统计模式识分类问题中，当先验概率未知时，可以使用( BC )?
A. 最小损失准则 
B. N-P判决 
C. 最小最大损失准则 
D. 最小误判概率准则
```
```
选项 A ,最小损失准则中需要用到先验概率

选项B ,在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。 
　　1. p(y)已知，直接使用贝叶斯公式求后验概率即可； 
　　2. p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。 
　　聂曼-皮尔逊决策（N-P判决）可以归结为找阈值a，即： 
　　如果p（x|w1）/p（x|w2）>a，则 x属于w1； 
　　如果p（x|w1）/p（x|w2）<a，则 x属于w2；

选项C ,最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的问题的。

```
## p10
```
下面哪些是基于核的机器学习算法?( BCD )
A. Expectation Maximization（EM）（最大期望算法） 
B. Radial Basis Function（RBF）（径向基核函数） 
C. Linear Discrimimate Analysis（LDA）（主成分分析法） 
D. Support Vector Machine（SVM）（支持向量机）
```
```
径向基核函数是非常常用的核函数，而主成分分析法的常规方法是线性的，但是当遇到非线性的时候，同样可以使用核方法使得非线性问题转化为线性问题。支持向量机处理非线性的问题的时候，核函数也是非常重要的。
```

## p11
```
对数几率回归（logistics regression）和一般回归分析有什么区别？ D
A. 对数几率回归是设计用来预测事件可能性的 
B. 对数几率回归可以用来度量模型拟合程度 
C. 对数几率回归可以用来估计回归系数 
D. 以上所有
```
```
A: 对数几率回归其实是设计用来解决分类问题的 
B: 对数几率回归可以用来检验模型对数据的拟合度 
C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。
```
## p12
```
在一个n维的空间中， 最好的检测outlier(离群点)的方法是：（C）
A. 作正态分布概率图 
B. 作盒形图 
C. 马氏距离 
D. 作散点图
```
```
马氏距离是基于卡方分布的，度量多元outlier离群点的统计方法。
```

## p13
```
“过拟合”只在监督学习中出现，在非监督学习中，没有”过拟合”，这是：（B）
A. 对的 
B. 错的 

```
```
答案：（B）
我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score）。
```

## p14
```
对于k折交叉验证, 以下对k的说法正确的是 :（D）

A. k越大, 不一定越好, 选择大的k会加大评估时间 
B. 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集) 
C. 在选择k时, 要最小化数据集之间的方差 
D. 以上所有
```
```
k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差
```
## p15
```
回归模型中存在多重共线性, 你如何解决这个问题？
1.去除这两个共线性变量 
2.我们可以先去除一个共线性变量 
3.计算VIF(方差膨胀因子), 采取相应措施 
4.为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归. 
以下哪些是对的：（D）

A. 1 
B. 2 
C. 2和3 
D. 2, 3和4
```
```
解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值<=4说明相关性不是很高, VIF值>=10说明相关性较高. 
我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。
```

## p16
```
假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值, 那么现在关于模型说法, 正确的是 : （C）

1.模型分类的召回率会降低或不变 
2.模型分类的召回率会升高 
3.模型分类准确率会升高或不变 
4.模型分类准确率会降低

A. 1 
B. 2 
C.1和3 
D. 2和4 
E. 以上都不是
```
- reference
Confidence Splitting Criterions Can Improve Precision And Recall in Random Forest Classifiers. https://medium.com/airbnb-engineering/confidence-splitting-criterions-can-improve-precision-and-recall-in-random-forest-classifiers-ad2d4ba696a4

## p17
```
对于PCA(主成分分析)转化过的特征 , 朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 :（B）

A. 正确的 
B. 错误的
```
```
这个说法是错误的。首先，“不依赖”和“不相关”是两回事；其次, 转化过的特征, 也可能是相关的。
```

## p18
```
对应GradientBoosting tree算法， 以下说法正确的是 :（C） 
1.当增加最小样本分裂个数，我们可以抵制过拟合 
2.当增加最小样本分裂个数，会导致过拟合 
3.当我们减少训练单个学习器的样本个数，我们可以降低variance 
4.当我们减少训练单个学习器的样本个数，我们可以降低bias
A. 2 和 4 
B. 2 和 3 
C. 1 和 3 
D. 1 和 4

```
```
最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。第二点是考bias和variance概念的。
```
## p19
```
对于线性回归模型，包括附加变量在内，以下的可能正确的是 :（D） 
1.R-Squared 和 Adjusted R-squared都是递增的 
2.R-Squared 是常量的，Adjusted R-squared是递增的 
3.R-Squared 是递减的， Adjusted R-squared 也是递减的 
4.R-Squared 是递减的， Adjusted R-squared是递增的

A. 1 和 2 
B. 1 和 3 
C. 2 和 4 
D. 以上都不是
```
```
R-Squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-Squared有R-Squared和predicted R-Squared所没有的问题。每次为模型加入预测器，R-Squared递增或者不变。
```

## p20
```
一 般的，如果相关系数大于0.7或者小于-0.7，是高相关的。
相关系数的范围应该是[-1,1]。
```
## p21
```
下面有关序列模式挖掘算法的描述，错误的是？（C）

A. AprioriAll算法和GSP算法都属于Apriori类算法，都要产生大量的候选序列 
B. FreeSpan算法和PrefixSpan算法不生成大量的候选序列以及不需要反复扫描原数据库 
C. 在时空的执行效率上，FreeSpan比PrefixSpan更优 
D. 和AprioriAll相比，GSP的执行效率比较高
```
```
2. GPS算法：类Apriori算法。用于从候选项集中发现具有时序先后性的频繁项集。两个步骤：进行自连接、进行剪枝。缺点：每次计算支持度，都需要扫描全部数据集；对序列模式很长的情况，由于其对应的短的序列模式规模太大，算法很难处理。

3. SPADE算法：改进的GPS算法，规避多次对数据集D进行全表扫描的问题。与GSP算法大体相同，多了一个ID_LIST记录，使得每一次的ID_LIST根据上一次的ID_LIST得到（从而得到支持度）。而ID_LIST的规模是随着剪枝的不断进行而缩小的。所以也就解决了GSP算法多次扫描数据集D问题。

4.  FreeSpan算法：即频繁模式投影的序列模式挖掘。核心思想是分治算法。基本思想为：利用频繁项递归地将序列数据库投影到更小的投影数据库集中，在每个投影数据库中生成子序列片断。这一过程对数据和待检验的频繁模式集进行了分割，并且将每一次检验限制在与其相符合的更小的投影数据库中。

优点：减少产生候选序列所需的开销。缺点：可能会产生许多投影数据库，开销很大，会产生很多的

5. PrefixSpan 算法：从FreeSpan中推导演化而来的。收缩速度比FreeSpan还要更快些。
```
- reference
机器学习：序列模式挖掘算法. https://blog.csdn.net/ztf312/article/details/50889238

## p22
```
类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是？(D)

A. 伪逆法-径向基（RBF）神经网络的训练算法，就是解决线性不可分的情况 
B. 基于二次准则的H-K算法：最小均方差准则下求得权矢量，二次准则解决非线性问题 
C. 势函数法－非线性 
D. 感知器算法－线性分类算法
```

## p23
```
下列哪些方法可以用来对高维数据进行降维（A B C D E F）

A. LASSO 
B. 主成分分析法 
C. 聚类分析 
D. 小波分析法 
E. 线性判别法 
F. 拉普拉斯特征映射

```
```
LASSO通过参数缩减达到降维的目的； 
PCA就不用说了； 
线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维； 
小波分析有一些变换的操作降低其他干扰可以看做是降维；
```
- reference
机器学习降维算法四：Laplacian Eigenmaps 拉普拉斯特征映射. https://blog.csdn.net/xbinworld/article/details/8855796

## p24
```
1、SVM对噪声（如来自其他分布的噪声样本）鲁棒       错
SVM（支持向量机）本身对噪声具有一定的鲁棒性，但实验证明，是当噪声率低于一定水平（如40%）时噪声对SVM没有太大影响，算法仍有效，但随着噪声率的不断增加，分类器的识别率会降低。http://www.docin.com/p-749158537.html

拓展：SVM在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。

在机器学习中，支持向量机（Support Vector Machine，还支持矢量网络）是与相关的学习算法有关的监督学习模型，可以分析数据，识别模式，用于分类和回归分析。

2、在AdaBoost算法中，所有被分错的样本的权重更新比例相同  对
AdaBoost算法中不同的训练集是通过调整每个样本对应的权重来实现的。开始时，每个样本对应的权重是相同的，即其中 n 为样本个数，在此样本分布下训练出一弱分类器。对于分类错误的样本，加大其对应的权重；而对于分类正确的样本，降低其权重，这样分错的样本就被突显出来，从而得到一个新的样本分布。在新的样本分布下，再次对样本进行训练，得到弱分类器。依次类推，经过 T 次循环，得到 T 个弱分类器，把这 T 个弱分类器按一定的权重叠加（boost）起来，得到最终想要的强分类器。
AdaBoost算法的具体步骤如下：

1. 给定训练样本集S，其中X和Y分别对应于正例样本和负例样本； T为训练的最大循环次数；

2. 初始化样本权重为1/n ，即为训练样本的初始概率分布；

3. 第一次迭代：

(1) 训练样本的概率分布相当下，训练弱分类器；

(2) 计算弱分类器的错误率；

(3) 选取合适阈值，使得误差最小；

(4) 更新样本权重；

经T次循环后，得到T个弱分类器，按更新的权重叠加，最终得到的强分类器。

Adaboost算法是经过调整的Boosting算法，其能够对弱学习得到的弱分类器的错误进行适应性调整。每一次迭代，都要对权重进行更新。更新的规则是：减小弱分类器分类效果较好的数据的概率，增大弱分类器分类效果较差的数据的概率。最终的分类器是个弱分类器的加权平均。

3、Boost和Bagging都是组合多个分类器投票的方法，二者均是根据单个分类器的正确率决定其权重          错 
 Boost和Bagging都是组合多个弱分类器投票的方法，但二者均是根据单个分类器的正确率决定其权重的说法错误

Bagging与Boosting的区别：二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的各轮训练集的选择与前面各轮的学习结果有关

投票方法（Voting，也叫组合分类器，使用在文本分类中）就是一种典型的集成机器学习方法。它通过组合多个弱分类器来得到一个强分类器，包括Bagging和Boosting两种方式，二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。投票分类方法虽然分类精度较高，但训练时间较长。

Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成。

拓展：bagging和boosting都可以有效地提高分类的准确性。在大多数数据集中，boosting的准确性比bagging高。在有些数据集中，boosting会引起退化--- Overfitting(过拟合)。

Boosting思想的一种改进型AdaBoost方法在邮件过滤、文本分类方面都有很好的性能。

4、给定n 个数据点，如果其中一半用于训练，一半用于测试，则训练误差与测试误差之间的差别会随着n 的增加而减少         对
```
- reference
搜狗2016研发工程师笔试题中有关于机器学习的几个判断题及解析. http://www.mamicode.com/info-detail-1208462.html

## p25
```
以下关于PMF(概率质量函数),PDF(概率密度函数),CDF(累积分布函数)描述错误的是？（A）

A. PDF描述的是连续型随机变量在特定取值区间的概率 
B. CDF是PDF在特定区间上的积分 
C. PMF描述的是离散型随机变量在特定取值点的概率 
D. 有一个分布的CDF函数H(x),则H(a)等于P(X<=a)
```
```
概率质量函数 (probability mass function，PMF)是离散随机变量在各特定取值上的概率。 
概率密度函数（probability density function，PDF ）是对连续随机变量定义的，本身不是概率，只有对连续随机变量的取值进行积分后才是概率。 
累积分布函数（cumulative distribution function，CDF） 能完整描述一个实数随机变量X的概率分布，是概率密度函数的积分。对于所有实数x 与pdf相对。
```
## p26
```
线性回归的基本假设有哪些？(ABDE)

A. 随机误差项是一个期望值为0的随机变量； 
B. 对于解释变量的所有观测值，随机误差项有相同的方差； 
C. 随机误差项彼此相关； 
D. 解释变量是确定性变量不是随机变量，与随机误差项之间相互独立； 
E. 随机误差项服从正态分布
```
## p27
```
下面哪些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是正确的？(A、C）

A. 类型 1 通常称之为假正类，类型 2 通常称之为假负类。 
B. 类型 2 通常称之为假正类，类型 1 通常称之为假负类。 
C. 类型 1 错误通常在其是正确的情况下拒绝假设而出现。
 
在统计学假设测试中，I 类错误即错误地拒绝了正确的假设即假正类错误，II 类错误通常指错误地接受了错误的假设即假负类错误。
```
## p28
```
模式为对数据集的全局性总结，它对整个测量空间的每一点做出描述；模型则对变量变化空间的一个有限区域做出描述。（错）
寻找模式和规则主要是对数据进行干扰，使其符合某种规则以及模式。（错）
离群点可以是合法的数据对象或者值。　　　　（对）
特征提取技术并不依赖于特定的领域。　　　　　　（错）
可视化技术对于分析的数据类型通常不是专用性的。　　　　（错）
DSS主要是基于数据仓库.联机数据分析和数据挖掘技术的应用。（对）
OLAP技术侧重于把数据库中的数据进行分析、转换成辅助决策信息，是继数据库技术发展之后迅猛发展起来的一种新技术。 （对）
数据仓库中间层OLAP服务器只能采用关系型OLAP （错）
数据仓库系统的组成部分包括数据仓库，仓库管理，数据抽取，分析工具等四个部分. (错)
Web数据挖掘是通过数据库中的一些属性来预测另一个属性,它在验证用户提出的假设过程中提取信息. （错）
关联规则挖掘过程是发现满足最小支持度的所有项集代表的规则。（错）
利用先验原理可以帮助减少频繁项集产生时需要探查的候选项个数（对）。
具有较高的支持度的项集具有较高的置信度。（错）
聚类（clustering）是这样的过程：它找出描述并区分数据类或概念的模型(或函数)，以便能够使用模型预测类标记未知的对象类。 （错）
分类(classification )是这样的过程：它找出描述并区分数据类或概念的模型(或函数)，以便能够使用模型预测类标记未知的对象类。
基于邻近度的离群点检测方法不能处理具有不同密度区域的数据集。（对）
如果一个对象不强属于任何簇，那么该对象是基于聚类的离群点。（对）
从点作为个体簇开始，每一步合并两个最接近的簇，这是一种分裂的层次聚类方法。（错）
DBSCAN是相对抗噪声的，并且能够处理任意形状和大小的簇。（对）
```

## p29
```
使用交互式的和可视化的技术，对数据进行探索属于数据挖掘的哪一类任务？
探索性数据分析

为数据的总体分布建模；把多维空间划分成组等问题属于数据挖掘的哪一类任务？
建模描述

建立一个模型，通过这个模型根据已知的变量值来预测其他某个变量值属于数据挖掘的哪一类任务？
预测建模

用户有一种感兴趣的模式并且希望在数据集中找到相似的模式，属于数据挖掘哪一类任务？
根据内容检索

假设12个销售价格记录已经排序，如下所示： 5,10,11,13,15,35,50,55,72,92,204,215 使用如下各方法将它们划分成三个箱。 (a)等频（等深）划分。 (b)等宽划分。 (c)聚类。

等深的箱: 箱1： 5, 10, 11, 13 箱2： 15, 35, 50, 55 箱3： 72, 92, 204, 215 
等宽划分的箱： (215-5 / 3) = 70
箱1： 5, 10, 11, 13, 15, 35, 50, 55, 72 箱2： 92 箱3： 204, 215 聚类的箱： 箱1： 5, 10, 11, 13, 15 箱2： 35, 50, 55, 72, 92 箱3： 204, 215

```
```
 只有非零值才重要的二元属性被称作：( C )

A 计数属性

B 离散属性

C非对称的二元属性

D 对称属性
```
```
下面不属于创建新属性的相关方法的是： (B)

A特征提取

B特征修改

C映射数据到新的空间

D特征构造

考虑值集{1、2、3、4、5、90}，其截断均值（p=20%）是 (3.5) ？

6* （20/2）%=0.6 约等于1
去掉1和90，剩下的算均值为（2+3+4+5）/4=3.5

截断均值 在统计学里面一般是去除最高端的5%和最低端的5%。当然为了满足不同的需求，不一定是5%，但是一般都是高端和低端同时去除同样比例的数据
目的主要是为了避免部分极高值和极低值对于数据整体均值的影响，从而使平均值对整体更加有代表性
```

## p30
```
假定用于分析的数据包含属性age。数据元组中age的值如下（按递增序）：13，15，16，16，19，20，20，21，22，22，25，25，25，30，33，33，35，35，36，40，45，46，52，70, 问题：使用按箱平均值平滑方法对上述数据进行平滑，箱的深度为3。第二个箱子

值为：(A)

A 18.3

B 22.6

C 26.8

D 27.9


```
- reference
数据挖掘笔记（三）—数据预处理. https://blog.csdn.net/lizhengnanhua/article/details/8982968

## p31
```
考虑值集{12， 24， 33， 24， 55， 68， 26}，其四分位数极差是 31
```
```
四分位数(Quartile)，即统计学中，把所有数值由小到大排列并分成四等份，处于三个分割点位置的数值就是四分位数。

Q1的位置= (n+1) × 0.25

Q2的位置= (n+1) × 0.5

Q3的位置= (n+1) × 0.75

n表示项数

第1个和第3个四分位数之间的距离是散布的一种简单度量，它给出被数据的中间一半所覆盖的范围。该距离称为四分位数极差(IQR)，定义为
IQR = Q3 - Q1

排序后：12 ，24, 24, 26，33 ,55,68

Q1位置：（7+1）*0.25 = 2 对应数24

Q3位置：（7+1)*0.75 = 6 对应数55

四分位数极差55-24 = 31

```

## p32
```
在抽样方法中，当合适的样本容量很难确定时，可以使用的抽样方法是： (D)

A 有放回的简单随机抽样

B 无放回的简单随机抽样

C 分层抽样

D 渐进抽样

数据仓库是随着时间变化的,下面的描述不正确的是 (C)

A. 数据仓库随时间的变化不断增加新的数据内容;

B. 捕捉到的新数据会覆盖原来的快照;

C. 数据仓库随事件变化不断删去旧的数据内容;

D. 数据仓库中包含大量的综合数据,这些综合数据会随着时间的变化不断地进行重新综合.


```

## p33
```
OLAP技术的核心是:多维分析.
关于OLAP的特性,下面正确的是:(1)快速性 (2)可分析性 (3)多维性 (4)信息性 (5)共享性
基本元数据包括关于装载和更新处理,分析处理以及管理方面的信息.
数据仓库开发要从数据出发;
数据仓库使用的需求在开发出去就要明确;
数据仓库的开发是一个不断循环的过程,是启发式的开发;
OLAM服务器通过用户图形借口接收用户的分析指令,在元数据的知道下,对超级立方体作一定的操作.
```
```
关于OLAP和OLTP的区别描述,不正确的是: (C)

A. OLAP主要是关于如何理解聚集的大量不同的数据.它与OTAP应用程序不同.

B. 与OLAP应用程序不同,OLTP应用程序包含大量相对简单的事务.

C. OLAP的特点在于事务量大,但事务内容比较简单且重复率高.

D. OLAP是以数据仓库为基础的,但其最终数据来源与OLTP一样均来自底层的数据库系统,两者面对的用户是相同的.

概念分层图是__(B)__图。

A、无向无环

B、有向无环

C、有向有环

D、无向有环

在图集合中发现一组公共子结构，这样的任务称为 ( B )

A、频繁子集挖掘

B、频繁子图挖掘

C、频繁数据项挖掘

D、频繁模式挖掘

下列度量不具有反演性的是 (D)

A、系数

B、几率

C、Cohen度量

D、兴趣因子

以下哪项关于决策树的说法是错误的 (C)

A. 冗余属性不会对决策树的准确率造成不利的影响

B. 子树可能在决策树中重复多次

C. 决策树算法对于噪声的干扰非常敏感

D. 寻找最佳决策树是NP完全问题

```
## p34
```
如果对属性值的任一组合，R中都存在一条规则加以覆盖，则称规则集R中的规则为(B)

A, 无序规则

B，穷举规则

C， 互斥规则

D，有序规则

如果允许一条记录触发多条分类规则，把每条被触发规则的后件看作是对相应类的一次投票，然后计票确定测试记录的类标号，称为（A）

A, 无序规则

B，穷举规则

C， 互斥规则

D，有序规则

```
```
凝聚层次聚类：

   所谓凝聚的，指的是该算法初始时，将每个点作为一个簇，每一步合并两个最接近的簇。另外即使到最后，对于噪音点或是离群点也往往还是各占一簇的，除非过度合并。对于这里的“最接近”，有下面三种定义。我在实现是使用了MIN，该方法在合并时，只要依次取当前最近的点对，如果这个点对当前不在一个簇中，将所在的两个簇合并就行：

    (1)单链(MIN):定义簇的邻近度为不同两个簇的两个最近的点之间的距离。

    (2)全链(MAX):定义簇的邻近度为不同两个簇的两个最远的点之间的距离。

    (3)组平均：定义簇的邻近度为取自两个不同簇的所有点对邻近度的平均值。

根据该算法，实现如下代码。开始时计算每个点对的距离，并按距离降序依次合并。另外为了防止过度合并，定义的退出条件是90%的簇被合并，即当前簇数是初始簇数的10%：
```
- reference
聚类算法:凝聚层次聚类. https://blog.csdn.net/yt_sports/article/details/50456859

## p35
```
说出物体检测、人脸识别、物体分割等某一领域的常见算法，并用一两句话简述其中一种算法的原理
```
```
物体检测：YOLO、R-CNN、Fast R-CNN、Faster R-CNN、Mask R-CNN、SSD
YOLO 算法直接利用一个卷积神经网络就输出物体所在的位置以及所属的类别，是一个端到端的系统，因此检测速度特别快，可以达到实时性的要求。
```
## p36
```
正则化方法
数据增强(Data Augmentation)、L1 正则化、L2 正则化、Dropout、Drop Connect 和早停(Early stop)
```
